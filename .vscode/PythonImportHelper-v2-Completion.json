[
    {
        "label": "asyncpg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncpg",
        "description": "asyncpg",
        "detail": "asyncpg",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "src.models.user_behavior",
        "description": "src.models.user_behavior",
        "isExtraImport": true,
        "detail": "src.models.user_behavior",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "src.models.user_behavior",
        "description": "src.models.user_behavior",
        "isExtraImport": true,
        "detail": "src.models.user_behavior",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "src.models.user_behavior",
        "description": "src.models.user_behavior",
        "isExtraImport": true,
        "detail": "src.models.user_behavior",
        "documentation": {}
    },
    {
        "label": "Event",
        "importPath": "src.models.user_behavior",
        "description": "src.models.user_behavior",
        "isExtraImport": true,
        "detail": "src.models.user_behavior",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "importPath": "pydantic",
        "description": "pydantic",
        "isExtraImport": true,
        "detail": "pydantic",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "essentia.standard",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "essentia.standard",
        "description": "essentia.standard",
        "detail": "essentia.standard",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "ffmpeg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ffmpeg",
        "description": "ffmpeg",
        "detail": "ffmpeg",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "importPath": "src.services.extract_track_features.essentia_features",
        "description": "src.services.extract_track_features.essentia_features",
        "isExtraImport": true,
        "detail": "src.services.extract_track_features.essentia_features",
        "documentation": {}
    },
    {
        "label": "predict",
        "importPath": "src.services.extract_track_features.predict",
        "description": "src.services.extract_track_features.predict",
        "isExtraImport": true,
        "detail": "src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "extract_openl3",
        "importPath": "src.services.extract_track_features.openl3_embedding",
        "description": "src.services.extract_track_features.openl3_embedding",
        "isExtraImport": true,
        "detail": "src.services.extract_track_features.openl3_embedding",
        "documentation": {}
    },
    {
        "label": "openl3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "openl3",
        "description": "openl3",
        "detail": "openl3",
        "documentation": {}
    },
    {
        "label": "joblib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "joblib",
        "description": "joblib",
        "detail": "joblib",
        "documentation": {}
    },
    {
        "label": "AIOKafkaConsumer",
        "importPath": "aiokafka",
        "description": "aiokafka",
        "isExtraImport": true,
        "detail": "aiokafka",
        "documentation": {}
    },
    {
        "label": "AIOKafkaProducer",
        "importPath": "aiokafka",
        "description": "aiokafka",
        "isExtraImport": true,
        "detail": "aiokafka",
        "documentation": {}
    },
    {
        "label": "insert_event",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "del_events_of_track_id",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "del_track_metadata",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "get_user_events",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "get_track_metadata",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "get_all_event_data",
        "importPath": "src.database.db",
        "description": "src.database.db",
        "isExtraImport": true,
        "detail": "src.database.db",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "asyncio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "asyncio",
        "description": "asyncio",
        "detail": "asyncio",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "importPath": "sklearn.metrics.pairwise",
        "description": "sklearn.metrics.pairwise",
        "isExtraImport": true,
        "detail": "sklearn.metrics.pairwise",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SVD",
        "importPath": "surprise",
        "description": "surprise",
        "isExtraImport": true,
        "detail": "surprise",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "surprise",
        "description": "surprise",
        "isExtraImport": true,
        "detail": "surprise",
        "documentation": {}
    },
    {
        "label": "Reader",
        "importPath": "surprise",
        "description": "surprise",
        "isExtraImport": true,
        "detail": "surprise",
        "documentation": {}
    },
    {
        "label": "load_matrices",
        "importPath": "src.services.recommendation",
        "description": "src.services.recommendation",
        "isExtraImport": true,
        "detail": "src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "get_recommendations",
        "importPath": "src.services.recommendation",
        "description": "src.services.recommendation",
        "isExtraImport": true,
        "detail": "src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "send_event_to_kafka",
        "importPath": "src.services.kafka.kafka_producer",
        "description": "src.services.kafka.kafka_producer",
        "isExtraImport": true,
        "detail": "src.services.kafka.kafka_producer",
        "documentation": {}
    },
    {
        "label": "consume_events",
        "importPath": "src.services.kafka.kafka_consumer",
        "description": "src.services.kafka.kafka_consumer",
        "isExtraImport": true,
        "detail": "src.services.kafka.kafka_consumer",
        "documentation": {}
    },
    {
        "label": "update_svd_model",
        "importPath": "src.services.update_model",
        "description": "src.services.update_model",
        "isExtraImport": true,
        "detail": "src.services.update_model",
        "documentation": {}
    },
    {
        "label": "delete_event_relative_track_id",
        "importPath": "src.services.eventServices",
        "description": "src.services.eventServices",
        "isExtraImport": true,
        "detail": "src.services.eventServices",
        "documentation": {}
    },
    {
        "label": "RotatingFileHandler",
        "importPath": "logging.handlers",
        "description": "logging.handlers",
        "isExtraImport": true,
        "detail": "logging.handlers",
        "documentation": {}
    },
    {
        "label": "extract",
        "importPath": "src.services.extract_track_features.extract",
        "description": "src.services.extract_track_features.extract",
        "isExtraImport": true,
        "detail": "src.services.extract_track_features.extract",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "logger = logging.getLogger(__name__)\nPOSTGRES_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\nPOSTGRES_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\nPOSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"your_user\")\nPOSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"your_password\")\nPOSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "POSTGRES_HOST",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "POSTGRES_HOST = os.getenv(\"POSTGRES_HOST\", \"localhost\")\nPOSTGRES_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\nPOSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"your_user\")\nPOSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"your_password\")\nPOSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(\n            user=POSTGRES_USER,",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "POSTGRES_PORT",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "POSTGRES_PORT = os.getenv(\"POSTGRES_PORT\", \"5432\")\nPOSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"your_user\")\nPOSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"your_password\")\nPOSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(\n            user=POSTGRES_USER,\n            password=POSTGRES_PASSWORD,",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "POSTGRES_USER",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "POSTGRES_USER = os.getenv(\"POSTGRES_USER\", \"your_user\")\nPOSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"your_password\")\nPOSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(\n            user=POSTGRES_USER,\n            password=POSTGRES_PASSWORD,\n            database=POSTGRES_DB,",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "POSTGRES_PASSWORD",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "POSTGRES_PASSWORD = os.getenv(\"POSTGRES_PASSWORD\", \"your_password\")\nPOSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(\n            user=POSTGRES_USER,\n            password=POSTGRES_PASSWORD,\n            database=POSTGRES_DB,\n            host=POSTGRES_HOST,",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "POSTGRES_DB",
        "kind": 5,
        "importPath": "recommender.src.database.db",
        "description": "recommender.src.database.db",
        "peekOfCode": "POSTGRES_DB = os.getenv(\"POSTGRES_DB\", \"your_database\")\nasync def get_db_connection():\n    logger.info(f\"Creating new database connection to {POSTGRES_DB} as {POSTGRES_USER}\")\n    try:\n        conn = await asyncpg.connect(\n            user=POSTGRES_USER,\n            password=POSTGRES_PASSWORD,\n            database=POSTGRES_DB,\n            host=POSTGRES_HOST,\n            port=POSTGRES_PORT,",
        "detail": "recommender.src.database.db",
        "documentation": {}
    },
    {
        "label": "Event",
        "kind": 6,
        "importPath": "recommender.src.models.user_behavior",
        "description": "recommender.src.models.user_behavior",
        "peekOfCode": "class Event(BaseModel):\n    event_id: str\n    event_type: str\n    track_id: int\n    user_id: str\n    timestamp: datetime",
        "detail": "recommender.src.models.user_behavior",
        "documentation": {}
    },
    {
        "label": "extract_features",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.essentia_features",
        "description": "recommender.src.services.extract_track_features.essentia_features",
        "peekOfCode": "def extract_features(audio, sample_rate=44100):\n    logger.info(\"Starting feature extraction; audio shape: %s\", audio.shape)\n    try:\n        if audio.ndim == 2:\n            logger.debug(\"Converting stereo to mono\")\n            audio = np.mean(audio, axis=1)\n        audio = audio.astype(np.float32)\n        duration_ms = int(len(audio) / sample_rate * 1000)\n        rms = es.RMS()(audio)\n        loudness = es.Loudness()(audio)",
        "detail": "recommender.src.services.extract_track_features.essentia_features",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.essentia_features",
        "description": "recommender.src.services.extract_track_features.essentia_features",
        "peekOfCode": "logger = logging.getLogger(__name__)\nkey_to_int = {\n    'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4,\n    'F': 5, 'F#': 6, 'G': 7, 'G#': 8, 'A': 9,\n    'A#': 10, 'B': 11\n}\ndef extract_features(audio, sample_rate=44100):\n    logger.info(\"Starting feature extraction; audio shape: %s\", audio.shape)\n    try:\n        if audio.ndim == 2:",
        "detail": "recommender.src.services.extract_track_features.essentia_features",
        "documentation": {}
    },
    {
        "label": "key_to_int",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.essentia_features",
        "description": "recommender.src.services.extract_track_features.essentia_features",
        "peekOfCode": "key_to_int = {\n    'C': 0, 'C#': 1, 'D': 2, 'D#': 3, 'E': 4,\n    'F': 5, 'F#': 6, 'G': 7, 'G#': 8, 'A': 9,\n    'A#': 10, 'B': 11\n}\ndef extract_features(audio, sample_rate=44100):\n    logger.info(\"Starting feature extraction; audio shape: %s\", audio.shape)\n    try:\n        if audio.ndim == 2:\n            logger.debug(\"Converting stereo to mono\")",
        "detail": "recommender.src.services.extract_track_features.essentia_features",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.extract",
        "description": "recommender.src.services.extract_track_features.extract",
        "peekOfCode": "def load_audio(file_path, target_sr=44100):\n    logger.info(\"Loading audio from %s\", file_path)\n    try:\n        out, _ = (\n            ffmpeg\n            .input(file_path)\n            .output('pipe:', format='wav', acodec='pcm_s16le', ar=target_sr)\n            .run(capture_stdout=True, capture_stderr=True)\n        )\n        audio_data, sr = sf.read(io.BytesIO(out))",
        "detail": "recommender.src.services.extract_track_features.extract",
        "documentation": {}
    },
    {
        "label": "file_exists_at_url",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.extract",
        "description": "recommender.src.services.extract_track_features.extract",
        "peekOfCode": "def file_exists_at_url(url):\n    logger.debug(\"Checking URL existence: %s\", url)\n    try:\n        response = requests.head(url, allow_redirects=True, timeout=5)\n        if response.status_code == 405:\n            response = requests.get(url, stream=True, timeout=5)\n        exists = response.status_code == 200\n        logger.debug(\"URL %s exists: %s\", url, exists)\n        return exists\n    except requests.RequestException as e:",
        "detail": "recommender.src.services.extract_track_features.extract",
        "documentation": {}
    },
    {
        "label": "extract",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.extract",
        "description": "recommender.src.services.extract_track_features.extract",
        "peekOfCode": "def extract(file_name):\n    logger.info(\"Starting extract for file: %s\", file_name)\n    backend_host = os.getenv(\"FASTAPI_BACKEND_HOST\", \"localhost\")\n    backend_port = os.getenv(\"FASTAPI_BACKEND_PORT\", \"8001\")\n    url = f\"http://{backend_host}:{backend_port}/assets/track_audio/{file_name}\"\n    if not file_exists_at_url(url):\n        msg = f\"Audio file not found: {url}\"\n        logger.error(msg)\n        raise FileNotFoundError(msg)\n    try:",
        "detail": "recommender.src.services.extract_track_features.extract",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.extract",
        "description": "recommender.src.services.extract_track_features.extract",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef load_audio(file_path, target_sr=44100):\n    logger.info(\"Loading audio from %s\", file_path)\n    try:\n        out, _ = (\n            ffmpeg\n            .input(file_path)\n            .output('pipe:', format='wav', acodec='pcm_s16le', ar=target_sr)\n            .run(capture_stdout=True, capture_stderr=True)\n        )",
        "detail": "recommender.src.services.extract_track_features.extract",
        "documentation": {}
    },
    {
        "label": "extract_openl3",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.openl3_embedding",
        "description": "recommender.src.services.extract_track_features.openl3_embedding",
        "peekOfCode": "def extract_openl3(audio, sample_rate, content_type=\"music\", embedding_size=512):\n    logger.info(\"Extracting OpenL3 embedding; audio length=%d, sr=%d\", len(audio), sample_rate)\n    try:\n        if audio.ndim == 2:\n            logger.debug(\"Averaging stereo channels for embedding\")\n            audio = audio.mean(axis=1)\n        audio = audio.astype(np.float32)\n        embedding, timestamps = openl3.get_audio_embedding(\n            audio, sample_rate,\n            content_type=content_type,",
        "detail": "recommender.src.services.extract_track_features.openl3_embedding",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.openl3_embedding",
        "description": "recommender.src.services.extract_track_features.openl3_embedding",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef extract_openl3(audio, sample_rate, content_type=\"music\", embedding_size=512):\n    logger.info(\"Extracting OpenL3 embedding; audio length=%d, sr=%d\", len(audio), sample_rate)\n    try:\n        if audio.ndim == 2:\n            logger.debug(\"Averaging stereo channels for embedding\")\n            audio = audio.mean(axis=1)\n        audio = audio.astype(np.float32)\n        embedding, timestamps = openl3.get_audio_embedding(\n            audio, sample_rate,",
        "detail": "recommender.src.services.extract_track_features.openl3_embedding",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.predict",
        "description": "recommender.src.services.extract_track_features.predict",
        "peekOfCode": "def load_model():\n    global Model\n    if Model is None:\n        model_path = os.path.join(os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\"), \"extractor_model.pkl\")\n        logger.info(\"Loading prediction model from %s\", model_path)\n        try:\n            Model = joblib.load(model_path)\n            logger.info(\"Model loaded successfully\")\n        except Exception as e:\n            logger.error(\"Failed to load model: %s\", e, exc_info=True)",
        "detail": "recommender.src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "recommender.src.services.extract_track_features.predict",
        "description": "recommender.src.services.extract_track_features.predict",
        "peekOfCode": "def predict(embedding_vector):\n    logger.info(\"Predicting with embedding vector length %d\", len(embedding_vector))\n    if Model is None:\n        load_model()\n    arr = np.array(embedding_vector)\n    if arr.ndim > 1:\n        arr = arr.flatten()\n    if arr.shape[0] != Model.n_features_in_:\n        msg = f\"Embedding size mismatch: expected {Model.n_features_in_}, got {arr.shape[0]}\"\n        logger.error(msg)",
        "detail": "recommender.src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.predict",
        "description": "recommender.src.services.extract_track_features.predict",
        "peekOfCode": "logger = logging.getLogger(__name__)\nModel = None\nLABELS = [\"explicit\", \"danceability\", \"speechiness\", \"acousticness\",\n          \"instrumentalness\", \"liveness\", \"valence\"]\ndef load_model():\n    global Model\n    if Model is None:\n        model_path = os.path.join(os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\"), \"extractor_model.pkl\")\n        logger.info(\"Loading prediction model from %s\", model_path)\n        try:",
        "detail": "recommender.src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.predict",
        "description": "recommender.src.services.extract_track_features.predict",
        "peekOfCode": "Model = None\nLABELS = [\"explicit\", \"danceability\", \"speechiness\", \"acousticness\",\n          \"instrumentalness\", \"liveness\", \"valence\"]\ndef load_model():\n    global Model\n    if Model is None:\n        model_path = os.path.join(os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\"), \"extractor_model.pkl\")\n        logger.info(\"Loading prediction model from %s\", model_path)\n        try:\n            Model = joblib.load(model_path)",
        "detail": "recommender.src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "LABELS",
        "kind": 5,
        "importPath": "recommender.src.services.extract_track_features.predict",
        "description": "recommender.src.services.extract_track_features.predict",
        "peekOfCode": "LABELS = [\"explicit\", \"danceability\", \"speechiness\", \"acousticness\",\n          \"instrumentalness\", \"liveness\", \"valence\"]\ndef load_model():\n    global Model\n    if Model is None:\n        model_path = os.path.join(os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\"), \"extractor_model.pkl\")\n        logger.info(\"Loading prediction model from %s\", model_path)\n        try:\n            Model = joblib.load(model_path)\n            logger.info(\"Model loaded successfully\")",
        "detail": "recommender.src.services.extract_track_features.predict",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_consumer",
        "description": "recommender.src.services.kafka.kafka_consumer",
        "peekOfCode": "logger = logging.getLogger(__name__)\nload_dotenv()\nKAFKA_HOST = os.getenv(\"KAFKA_HOST\", \"kafka\")\nKAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def consume_events():\n    logger.info(\"Initializing Kafka consumer...\")\n    try:\n        consumer = AIOKafkaConsumer(\n            'event_tracking_topic',",
        "detail": "recommender.src.services.kafka.kafka_consumer",
        "documentation": {}
    },
    {
        "label": "KAFKA_HOST",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_consumer",
        "description": "recommender.src.services.kafka.kafka_consumer",
        "peekOfCode": "KAFKA_HOST = os.getenv(\"KAFKA_HOST\", \"kafka\")\nKAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def consume_events():\n    logger.info(\"Initializing Kafka consumer...\")\n    try:\n        consumer = AIOKafkaConsumer(\n            'event_tracking_topic',\n            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n            group_id=\"event_tracking_group\",",
        "detail": "recommender.src.services.kafka.kafka_consumer",
        "documentation": {}
    },
    {
        "label": "KAFKA_PORT",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_consumer",
        "description": "recommender.src.services.kafka.kafka_consumer",
        "peekOfCode": "KAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def consume_events():\n    logger.info(\"Initializing Kafka consumer...\")\n    try:\n        consumer = AIOKafkaConsumer(\n            'event_tracking_topic',\n            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n            group_id=\"event_tracking_group\",\n            auto_offset_reset='earliest'",
        "detail": "recommender.src.services.kafka.kafka_consumer",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_consumer",
        "description": "recommender.src.services.kafka.kafka_consumer",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def consume_events():\n    logger.info(\"Initializing Kafka consumer...\")\n    try:\n        consumer = AIOKafkaConsumer(\n            'event_tracking_topic',\n            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n            group_id=\"event_tracking_group\",\n            auto_offset_reset='earliest'\n        )",
        "detail": "recommender.src.services.kafka.kafka_consumer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_producer",
        "description": "recommender.src.services.kafka.kafka_producer",
        "peekOfCode": "logger = logging.getLogger(__name__)\nKAFKA_HOST = os.getenv(\"KAFKA_HOST\", \"localhost\")\nKAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def send_event_to_kafka(event: Event, topic: str = \"event_tracking_topic\"):\n    logger.info(f\"Sending event {event.event_id} to Kafka topic: {topic}\")\n    try:\n        producer = AIOKafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n        logger.info(\"Starting Kafka producer...\")\n        await producer.start()",
        "detail": "recommender.src.services.kafka.kafka_producer",
        "documentation": {}
    },
    {
        "label": "KAFKA_HOST",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_producer",
        "description": "recommender.src.services.kafka.kafka_producer",
        "peekOfCode": "KAFKA_HOST = os.getenv(\"KAFKA_HOST\", \"localhost\")\nKAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def send_event_to_kafka(event: Event, topic: str = \"event_tracking_topic\"):\n    logger.info(f\"Sending event {event.event_id} to Kafka topic: {topic}\")\n    try:\n        producer = AIOKafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n        logger.info(\"Starting Kafka producer...\")\n        await producer.start()\n        event_data = event.json().encode(\"utf-8\")  # Pydantic tự serialize datetime",
        "detail": "recommender.src.services.kafka.kafka_producer",
        "documentation": {}
    },
    {
        "label": "KAFKA_PORT",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_producer",
        "description": "recommender.src.services.kafka.kafka_producer",
        "peekOfCode": "KAFKA_PORT = os.getenv(\"KAFKA_PORT\", \"9092\")\nKAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def send_event_to_kafka(event: Event, topic: str = \"event_tracking_topic\"):\n    logger.info(f\"Sending event {event.event_id} to Kafka topic: {topic}\")\n    try:\n        producer = AIOKafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n        logger.info(\"Starting Kafka producer...\")\n        await producer.start()\n        event_data = event.json().encode(\"utf-8\")  # Pydantic tự serialize datetime\n        logger.debug(f\"Event data size: {len(event_data)} bytes\")",
        "detail": "recommender.src.services.kafka.kafka_producer",
        "documentation": {}
    },
    {
        "label": "KAFKA_BOOTSTRAP_SERVERS",
        "kind": 5,
        "importPath": "recommender.src.services.kafka.kafka_producer",
        "description": "recommender.src.services.kafka.kafka_producer",
        "peekOfCode": "KAFKA_BOOTSTRAP_SERVERS = f\"{KAFKA_HOST}:{KAFKA_PORT}\"\nasync def send_event_to_kafka(event: Event, topic: str = \"event_tracking_topic\"):\n    logger.info(f\"Sending event {event.event_id} to Kafka topic: {topic}\")\n    try:\n        producer = AIOKafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n        logger.info(\"Starting Kafka producer...\")\n        await producer.start()\n        event_data = event.json().encode(\"utf-8\")  # Pydantic tự serialize datetime\n        logger.debug(f\"Event data size: {len(event_data)} bytes\")\n        await producer.send_and_wait(topic, event_data)",
        "detail": "recommender.src.services.kafka.kafka_producer",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.eventServices",
        "description": "recommender.src.services.eventServices",
        "peekOfCode": "logger = logging.getLogger(__name__)\nasync def delete_event_relative_track_id(track_id):\n    try:\n        res_del_events, res_del_track_metadata = await asyncio.gather(\n            del_events_of_track_id(track_id),\n            del_track_metadata(track_id)\n        )\n    except Exception as e:\n        # Log lỗi cụ thể nếu cần\n        logger.error(f\"Error deleting related data for track {track_id}: {e}\")",
        "detail": "recommender.src.services.eventServices",
        "documentation": {}
    },
    {
        "label": "load_matrices",
        "kind": 2,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "def load_matrices():\n    global P, Q, user_ids, track_ids\n    logger.info(\"Loading SVD matrices...\")\n    try:\n        matrix_folder = os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\")\n        P = joblib.load(os.path.join(matrix_folder, \"P_matrix.pkl\"))\n        Q = joblib.load(os.path.join(matrix_folder, \"Q_matrix.pkl\"))\n        user_ids = joblib.load(os.path.join(matrix_folder, \"user_ids.pkl\"))\n        track_ids = joblib.load(os.path.join(matrix_folder, \"track_ids.pkl\"))\n        logger.info(f\"Loaded matrices: P shape {P.shape}, Q shape {Q.shape}\")",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "svd_recommend",
        "kind": 2,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "def svd_recommend(user_id: str, n: int = 10):\n    global P, Q, user_ids, track_ids\n    logger.info(f\"Generating SVD recommendations for user: {user_id}\")\n    try:\n        if P is None or Q is None or user_ids is None or track_ids is None:\n            logger.info(\"Matrices not loaded, loading now...\")\n            load_matrices()\n        if user_id not in user_ids:\n            logger.warning(f\"User ID {user_id} not found in SVD model\")\n            raise ValueError(\"User ID not found in SVD model\")",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "logger = logging.getLogger(__name__)\nNUMERIC_FEATURES = [\n    'explicit',\n    'danceability',\n    'energy',\n    'key',\n    'loudness',\n    'mode',\n    'speechiness',\n    'acousticness',",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "NUMERIC_FEATURES",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "NUMERIC_FEATURES = [\n    'explicit',\n    'danceability',\n    'energy',\n    'key',\n    'loudness',\n    'mode',\n    'speechiness',\n    'acousticness',\n    'instrumentalness',",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "track_metadata",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "track_metadata = None\ntrack_vectors = None\nasync def load_track_metadata():\n    global track_metadata, track_vectors\n    logger.info(\"Loading track metadata...\")\n    try:\n        data = await get_track_metadata()\n        # Convert SQLAlchemy Row/Record objects to dicts\n        if data and not isinstance(data[0], dict):\n           data = [dict(row) for row in data]",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "track_vectors",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "track_vectors = None\nasync def load_track_metadata():\n    global track_metadata, track_vectors\n    logger.info(\"Loading track metadata...\")\n    try:\n        data = await get_track_metadata()\n        # Convert SQLAlchemy Row/Record objects to dicts\n        if data and not isinstance(data[0], dict):\n           data = [dict(row) for row in data]\n        logger.info(f\"Retrieved {len(data)} tracks from database\")",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "P",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "P = None\nQ = None\nuser_ids = None\ntrack_ids = None\ndef load_matrices():\n    global P, Q, user_ids, track_ids\n    logger.info(\"Loading SVD matrices...\")\n    try:\n        matrix_folder = os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\")\n        P = joblib.load(os.path.join(matrix_folder, \"P_matrix.pkl\"))",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "Q",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "Q = None\nuser_ids = None\ntrack_ids = None\ndef load_matrices():\n    global P, Q, user_ids, track_ids\n    logger.info(\"Loading SVD matrices...\")\n    try:\n        matrix_folder = os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\")\n        P = joblib.load(os.path.join(matrix_folder, \"P_matrix.pkl\"))\n        Q = joblib.load(os.path.join(matrix_folder, \"Q_matrix.pkl\"))",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "user_ids",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "user_ids = None\ntrack_ids = None\ndef load_matrices():\n    global P, Q, user_ids, track_ids\n    logger.info(\"Loading SVD matrices...\")\n    try:\n        matrix_folder = os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\")\n        P = joblib.load(os.path.join(matrix_folder, \"P_matrix.pkl\"))\n        Q = joblib.load(os.path.join(matrix_folder, \"Q_matrix.pkl\"))\n        user_ids = joblib.load(os.path.join(matrix_folder, \"user_ids.pkl\"))",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "track_ids",
        "kind": 5,
        "importPath": "recommender.src.services.recommendation",
        "description": "recommender.src.services.recommendation",
        "peekOfCode": "track_ids = None\ndef load_matrices():\n    global P, Q, user_ids, track_ids\n    logger.info(\"Loading SVD matrices...\")\n    try:\n        matrix_folder = os.getenv(\"FASTAPI_MATRIX_FOLDER\", \"static/matrix/\")\n        P = joblib.load(os.path.join(matrix_folder, \"P_matrix.pkl\"))\n        Q = joblib.load(os.path.join(matrix_folder, \"Q_matrix.pkl\"))\n        user_ids = joblib.load(os.path.join(matrix_folder, \"user_ids.pkl\"))\n        track_ids = joblib.load(os.path.join(matrix_folder, \"track_ids.pkl\"))",
        "detail": "recommender.src.services.recommendation",
        "documentation": {}
    },
    {
        "label": "process_data",
        "kind": 2,
        "importPath": "recommender.src.services.update_model",
        "description": "recommender.src.services.update_model",
        "peekOfCode": "def process_data(data):\n    logger.info(\"Processing event data...\")\n    try:\n        # Đếm số lần xuất hiện của từng (user_id, track_id, event_type)\n        count_dict = {}\n        for row in data:\n            user_id, track_id, event_type = row\n            key = (user_id, track_id, event_type)\n            count_dict[key] = count_dict.get(key, 0) + 1 # Nếu chưa tồn tại key trong count_dict, get(key, 0) trả về 0\n        # Tính điểm tương tác",
        "detail": "recommender.src.services.update_model",
        "documentation": {}
    },
    {
        "label": "create_dataframe",
        "kind": 2,
        "importPath": "recommender.src.services.update_model",
        "description": "recommender.src.services.update_model",
        "peekOfCode": "def create_dataframe(score_dict): # score_dict là một dict dạng {(user_id, track_id): score}\n    try:\n        df = pd.DataFrame(list(score_dict.items()), columns=[\"key\", \"score\"])\n        df[[\"user_id\", \"track_id\"]] = pd.DataFrame(df[\"key\"].tolist(), index=df.index)\n        # Dùng pivot để tạo ma trận:\n        # index=\"user_id\" → mỗi dòng là một bài hát\n        # columns=\"track_id\" → mỗi cột là một người dùng\n        # values=\"score\" → giá trị là điểm tương tác\n        # fillna(0) để thay giá trị thiếu bằng 0 (người dùng chưa tương tác)\n        df = df.pivot(index=\"user_id\", columns=\"track_id\", values=\"score\").fillna(0)",
        "detail": "recommender.src.services.update_model",
        "documentation": {}
    },
    {
        "label": "train_svd_and_save_matrices",
        "kind": 2,
        "importPath": "recommender.src.services.update_model",
        "description": "recommender.src.services.update_model",
        "peekOfCode": "def train_svd_and_save_matrices(df, k=10):\n    logger.info(\"Training SVD model...\")\n    try:\n        reader = Reader(rating_scale=(0, df.values.max()))\n        data = Dataset.load_from_df(df.stack().reset_index(), reader)\n        trainset = data.build_full_trainset()\n        logger.info(f\"Training set: {trainset.n_users} users, {trainset.n_items} items, {trainset.n_ratings} ratings\")\n        random_state = int(os.getenv(\"FASTAPI_RANDOM_STATE\", 42))\n        svd = SVD(n_factors=k, random_state=random_state)\n        svd.fit(trainset)",
        "detail": "recommender.src.services.update_model",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.src.services.update_model",
        "description": "recommender.src.services.update_model",
        "peekOfCode": "logger = logging.getLogger(__name__)\nSCORE_MAP: Dict[str, int] = {\n    \"click\": 1,\n    \"play\": 2\n}\ndef process_data(data):\n    logger.info(\"Processing event data...\")\n    try:\n        # Đếm số lần xuất hiện của từng (user_id, track_id, event_type)\n        count_dict = {}",
        "detail": "recommender.src.services.update_model",
        "documentation": {}
    },
    {
        "label": "TrackRequest",
        "kind": 6,
        "importPath": "recommender.main",
        "description": "recommender.main",
        "peekOfCode": "class TrackRequest(BaseModel):\n    track_file_name: str\n    track_id: str\n@app.post(\"/api/add_track\")\ndef add_track_feature(track_data: TrackRequest):\n    res = extract(track_data.track_file_name)\n    # track_file_name = track_data.track_file_name\n    # track_id = track_data.track_id\n    return {\"res\": {res}}\n    # return {\"res\": {track_file_name,track_id}}",
        "detail": "recommender.main",
        "documentation": {}
    },
    {
        "label": "add_track_feature",
        "kind": 2,
        "importPath": "recommender.main",
        "description": "recommender.main",
        "peekOfCode": "def add_track_feature(track_data: TrackRequest):\n    res = extract(track_data.track_file_name)\n    # track_file_name = track_data.track_file_name\n    # track_id = track_data.track_id\n    return {\"res\": {res}}\n    # return {\"res\": {track_file_name,track_id}}\n@app.delete(\"/api/delete_track/{id}\")\nasync def delete_track_feature(id: int):\n    try:\n        logger.info(f\"Received request to delete track with ID: {id}\")",
        "detail": "recommender.main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "recommender.main",
        "description": "recommender.main",
        "peekOfCode": "app = FastAPI()\n# Cấu hình logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n    handlers=[\n        RotatingFileHandler(\"app.log\", maxBytes=10000000, backupCount=5),\n        logging.StreamHandler()\n    ]\n)",
        "detail": "recommender.main",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "recommender.main",
        "description": "recommender.main",
        "peekOfCode": "logger = logging.getLogger(__name__)\n@app.on_event(\"startup\")\nasync def startup_event():\n    logger.info(\"Application starting up...\")\n    asyncio.create_task(consume_events())\n    logger.info(\"Kafka consumer background task started\")\n@app.post(\"/api/event_tracking\")\nasync def track_event(event: Event):\n    logger.info(f\"Tracking event: {event.event_id}\")\n    try:",
        "detail": "recommender.main",
        "documentation": {}
    }
]